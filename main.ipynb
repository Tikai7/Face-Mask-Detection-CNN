{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy,Mean\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "from tensorflow.keras.losses import BinaryCrossentropy\n",
    "from tensorflow.keras.metrics import BinaryAccuracy\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BS = 32\n",
    "EPOCHS = 5\n",
    "L_R = 1e-4\n",
    "\n",
    "STEP_PER_EPOCH = 2792 //BS #---- len(dataset_train)//BS\n",
    "STEP_PER_VAL_EPOCH = 1300 //BS #---len(dataset_test)//BS\n",
    "\n",
    "\n",
    "dict_architecture = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loadin & Preprocessing Dataset with ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "datatest = ImageDataGenerator(\n",
    "    preprocessing_function = preprocess_input\n",
    ")\n",
    "datagen = ImageDataGenerator (\n",
    "    preprocessing_function = preprocess_input,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.15,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.15,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\"\n",
    ")\n",
    "\n",
    "X_train = datagen.flow_from_directory(\n",
    "    \"dataset_2/train_set\",\n",
    "    target_size=(224,224),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=BS,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=True,\n",
    "    seed=213\n",
    ")\n",
    "\n",
    "X_test = datatest.flow_from_directory(\n",
    "    \"dataset_2/test_set\",\n",
    "    target_size=(224,224),\n",
    "    color_mode='grayscale',\n",
    "    batch_size=BS,\n",
    "    class_mode=\"sparse\",\n",
    "    shuffle=True,\n",
    "    seed=213,\n",
    ")\n",
    "\n",
    "#\n",
    "    # i=0\n",
    "    # for X,y in X_train:\n",
    "    #     for image in X:\n",
    "    #         plt.figure()\n",
    "    #         plt.imshow(image)\n",
    "    #         print(\"label : \",y[i])\n",
    "    #         i+=1\n",
    "    #         # print(\"Max : \",image.max())\n",
    "    #         # print(\"Min : \",image.min())\n",
    "    #         # print(\"Ecart Type : \",image.std())\n",
    "    #         # print(\"Moyenne : \",image.mean())\n",
    "    #         if i==5:\n",
    "    #             break\n",
    "    #     break\n",
    "\n",
    "    # i=0\n",
    "    # print(\"----------------- TEST --------------------\")\n",
    "    # for X,y in X_test:\n",
    "    #     for image in X:\n",
    "    #         plt.figure()\n",
    "    #         plt.imshow(image)\n",
    "    #         print(\"label : \",y[i])\n",
    "    #         i+=1\n",
    "    #         # print(\"Max : \",image.max())\n",
    "    #         # print(\"Min : \",image.min())\n",
    "    #         # print(\"Ecart Type : \",image.std())\n",
    "    #         # print(\"Moyenne : \",image.mean())\n",
    "    #         if i==10:\n",
    "    #             break\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading & Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_Temp = open('X_Data','rb')\n",
    "# X_Data = pickle.load(X_Temp)\n",
    "\n",
    "# Y_Temp = open('Y_Data','rb')\n",
    "# Y_Data = pickle.load(Y_Temp)\n",
    "\n",
    "# X_Data = np.array(X_Data)\n",
    "# Y_Data = np.array(Y_Data)\n",
    "\n",
    "# print(X_Data.shape)\n",
    "# print(Y_Data.shape)\n",
    "\n",
    "\n",
    "# X_train,X_test,y_train,y_test = train_test_split(X_Data,Y_Data,test_size=0.2,random_state=42,shuffle=True)\n",
    "\n",
    "# X_train = X_train.astype('float32')\n",
    "# X_test = X_test.astype('float32')\n",
    "\n",
    "# def augment(image):\n",
    "#   image = tf.image.random_flip_left_right(image)\n",
    "#   return image\n",
    "\n",
    "# for i in range(len(X_train)):\n",
    "#   X_train[i] = augment(X_train[i])\n",
    "\n",
    "\n",
    "# for i in range(5):\n",
    "#   plt.figure()\n",
    "#   print(y_train[i])\n",
    "#   plt.imshow(X_train[i])\n",
    "#   plt.show()\n",
    "\n",
    "\n",
    "# # ---------------------------------------- Normalisation MinMax\n",
    "# scaler = MinMaxScaler()\n",
    "# X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],-1))\n",
    "# X_test = scaler.transform(X_test.reshape(X_test.shape[0],-1))\n",
    "# pickle.dump(scaler, open('scaler.pkl', 'wb'))\n",
    "# #---------------------------------------- Normalisation StandarScaler\n",
    "# # scaler = StandardScaler()\n",
    "# # X_train = scaler.fit_transform(X_train.reshape(X_train.shape[0],-1))\n",
    "# # X_test = scaler.transform(X_test.reshape(X_test.shape[0],-1))\n",
    "# # pickle.dump(scaler, open('scaler.pkl', 'wb'))\n",
    "\n",
    "\n",
    "# X_train = X_train.reshape(-1,224,224,1) \n",
    "# X_test = X_test.reshape(-1,224,224,1) \n",
    "\n",
    "# print(X_train.max())\n",
    "# print(X_test.max())\n",
    "# print(X_test.mean())\n",
    "# print(X_test.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------ Bloc TikaiNet -----------------------\n",
    "\n",
    "class TikaiNetBlock(layers.Layer):\n",
    "    \n",
    "    def __init__(self,filters,strides,number,id_block,**kwargs):\n",
    "        super(TikaiNetBlock,self).__init__()\n",
    "        self.number = number\n",
    "        self.id = id_block\n",
    "        self.dw = layers.DepthwiseConv2D(kernel_size=3,strides=strides,padding=\"same\",name=f\"dw_id{id_block}\")\n",
    "        self.bn = layers.BatchNormalization(name=f\"bn_id{id_block}\")\n",
    "        self.bn_2 = layers.BatchNormalization(name=f\"bn_2_id{id_block}\")\n",
    "        self.relu = layers.ReLU(name=f\"relu_id{id_block}\")\n",
    "        self.relu_2 = layers.ReLU(name=f\"relu_2_id{id_block}\")\n",
    "        self.conv_2 = layers.Conv2D(filters=filters,kernel_size=1,strides=1,name=f\"conv_2_id{id_block}\",padding=\"same\")\n",
    "        self.sc_2 = layers.SeparableConv2D(filters=filters,kernel_size=1,strides=1,name=f\"sc_2_id{id_block}\",padding=\"same\")\n",
    "        self.drop = layers.Dropout(0.5)\n",
    "\n",
    "    def call(self,X):\n",
    "        \n",
    "        X = self.dw(X)\n",
    "        X = self.relu(X)\n",
    "        X = self.bn(X)\n",
    "\n",
    "        if self.number%2==0:\n",
    "            dict_architecture[self.id] = self.number\n",
    "            X = self.conv_2(X)\n",
    "        else:\n",
    "            dict_architecture[self.id] = self.number\n",
    "            X = self.sc_2(X)\n",
    "\n",
    "\n",
    "        X = self.relu_2(X)\n",
    "        X = self.bn_2(X)\n",
    "        return X\n",
    "    \n",
    "\n",
    "#------------------ Model TikaiNet -----------------------\n",
    "\n",
    "class TikaiNetV1(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(TikaiNetV1,self).__init__()\n",
    "        self.dw_s1_64  = TikaiNetBlock(64,1,2,id_block=1)\n",
    "        self.dw_s2_128 = TikaiNetBlock(128,2,2,id_block=2)\n",
    "        self.dw_s1_128 = TikaiNetBlock(128,1,2,id_block=3)\n",
    "        self.dw_s2_256 = TikaiNetBlock(256,2,2,id_block=4)\n",
    "        self.dw_s1_256 = TikaiNetBlock(256,1,2,id_block=5)\n",
    "        self.dw_s2_512 = TikaiNetBlock(512,2,2,id_block=6)\n",
    "\n",
    "        self.dw_s1_512_1 = TikaiNetBlock(512,1,2,id_block=7)\n",
    "        self.dw_s1_512_2 = TikaiNetBlock(512,1,2,id_block=8)\n",
    "        self.dw_s1_512_3 = TikaiNetBlock(512,1,2,id_block=9)\n",
    "        self.dw_s1_512_4 = TikaiNetBlock(512,1,2,id_block=10)\n",
    "        self.dw_s1_512_5 = TikaiNetBlock(512,1,2,id_block=11)\n",
    "\n",
    "        self.dw_s2_1024 = TikaiNetBlock(1024,2,2,id_block=12)\n",
    "        self.dw_s1_1024 = TikaiNetBlock(1024,1,2,id_block=13)\n",
    "\n",
    "       \n",
    "                \n",
    "    def call(self,X):\n",
    "        X =  self.dw_s1_64(X)\n",
    "        \n",
    "        X =  self.dw_s2_128(X)\n",
    "        X =  self.dw_s1_128(X)\n",
    "        X =  self.dw_s2_256(X)\n",
    "        X =  self.dw_s1_256(X)\n",
    "        X =  self.dw_s2_512(X)\n",
    "        \n",
    "        X = self.dw_s1_512_1(X)\n",
    "        X = self.dw_s1_512_2(X)  \n",
    "        X = self.dw_s1_512_3(X)\n",
    "        X = self.dw_s1_512_4(X)\n",
    "        X = self.dw_s1_512_5(X)\n",
    "\n",
    "        X =  self.dw_s2_1024(X)\n",
    "        X =  self.dw_s1_1024(X)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compiling & Training & Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tikai = TikaiNetV1()\n",
    "# Tikai.build((None,112,112,32))\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(32,3,2,padding=\"same\",input_shape=(224,224,1),name=\"first_conv\"))\n",
    "#model.add(layers.SeparableConv2D(32,3,2,padding=\"same\",input_shape=(224,224,1),name=\"first_Sconv\"))\n",
    "model.add(layers.ReLU(name=\"first_relu\"))\n",
    "model.add(layers.BatchNormalization(name=\"first_bn\"))\n",
    "\n",
    "\n",
    "#model.add(Tikai)\n",
    "#model.add(TikaiNetBlock(64,2,1,id_block=0))\n",
    "model.add(TikaiNetBlock(128,2,1,id_block=2))\n",
    "model.add(TikaiNetBlock(256,2,1,id_block=4))\n",
    "model.add(TikaiNetBlock(512,2,1,id_block=6))\n",
    "model.add(TikaiNetBlock(1024,2,1,id_block=12))\n",
    "\n",
    "#model.add(layers.Dropout(0.5))\n",
    "\n",
    "model.add(TikaiNetBlock(64,1,2,id_block=1))\n",
    "model.add(TikaiNetBlock(128,1,2,id_block=3))\n",
    "model.add(TikaiNetBlock(256,1,2,id_block=5))\n",
    "\n",
    "for i in range(5):\n",
    "    model.add(TikaiNetBlock(512,1,2,id_block=(7+i)))\n",
    "\n",
    "model.add(TikaiNetBlock(1024,1,2,id_block=13))\n",
    "\n",
    "\n",
    "model.add(layers.AveragePooling2D(pool_size=7,strides=1,name=\"avg_pooling\"))\n",
    "model.add(layers.Flatten(name=\"flatten\"))\n",
    "\n",
    "model.add(layers.Dense(128))\n",
    "model.add(layers.ReLU())\n",
    "#model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(2,name=\"last_dense\"))\n",
    "model.add(layers.Softmax())\n",
    "#model.add(layers.Dense(1,activation=\"sigmoid\",name=\"last_dense\"))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "with open('model_summary.txt', 'w') as f:\n",
    "    model.summary(print_fn=lambda x: f.write(x + '\\n'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Step Mode Eager (Slower-Easier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.compile(optimizer= Adam(learning_rate=L_R),\n",
    "#               loss = BinaryCrossentropy(),\n",
    "#               metrics=['accuracy'])\n",
    "\n",
    "# history = model.fit (\n",
    "#     X_train, y_train, batch_size=BS,\n",
    "#     validation_data=(X_test, y_test),\n",
    "#     epochs = EPOCHS,\n",
    "# )\n",
    "\n",
    "# model.save(\"TikaiNetV1\",save_format=\"tf\")\n",
    "\n",
    "\n",
    "# print(history.history.keys())\n",
    "\n",
    "# val_loss_curve =  history.history[\"val_loss\"]\n",
    "# val_acc_curve = history.history[\"val_accuracy\"]\n",
    "# acc_curve = history.history[\"accuracy\"]\n",
    "# loss_curve = history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Step Mode Graph (Faster-Harder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_of = SparseCategoricalCrossentropy()\n",
    "#loss_of = BinaryCrossentropy()\n",
    "optimizer = Adam(learning_rate=L_R, decay=L_R / EPOCHS)\n",
    "#optimizer = SGD()\n",
    "\n",
    "train_loss = Mean(name=\"train_loss\")\n",
    "train_acc = SparseCategoricalAccuracy(name=\"train_acc\")\n",
    "#train_acc = BinaryAccuracy(name=\"train_acc\")\n",
    "\n",
    "val_loss = Mean(name=\"valid_loss\")\n",
    "val_acc = SparseCategoricalAccuracy(name=\"valid_acc\")\n",
    "#val_acc = BinaryAccuracy(name=\"valid_acc\")\n",
    "\n",
    "model.compile(optimizer = optimizer,loss = loss_of,metrics=[SparseCategoricalAccuracy()])\n",
    "\n",
    "\n",
    "loss_curve = []\n",
    "acc_curve = []\n",
    "val_loss_curve = []\n",
    "val_acc_curve = []\n",
    "\n",
    "@tf.function\n",
    "def train_step(X,y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        prediction = model(X,training=True)\n",
    "        loss = loss_of(y,prediction)\n",
    "\n",
    "    gradients = tape.gradient(loss,model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(gradients,model.trainable_weights))\n",
    "    train_loss.update_state(loss)\n",
    "    train_acc.update_state(y,prediction)\n",
    "\n",
    "@tf.function\n",
    "def validation_step(X,y):\n",
    "    prediction = model(X,training=False)\n",
    "    loss = loss_of(y,prediction)\n",
    "    val_loss.update_state(loss)\n",
    "    val_acc.update_state(y,prediction)\n",
    "\n",
    "\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    nb_batchs = 0\n",
    "    nb_batchs_val = 0\n",
    "    # Training set\n",
    "    for X, y in X_train:\n",
    "        nb_batchs+=1\n",
    "        train_step(X, y)\n",
    "        print(f'Epoch: {epoch+1}/{EPOCHS} Batch: {nb_batchs}/{STEP_PER_EPOCH} loss: {train_loss.result()}, accuracy: {train_acc.result()}')\n",
    "        if nb_batchs == STEP_PER_EPOCH:\n",
    "            break\n",
    "\n",
    "    # Validation set\n",
    "    for X, y in X_test:\n",
    "        nb_batchs_val+=1\n",
    "        validation_step(X, y)\n",
    "        if nb_batchs_val == STEP_PER_VAL_EPOCH:\n",
    "            break\n",
    "       \n",
    "    print(f'Epoch: {epoch+1}/{EPOCHS} val_Loss: {val_loss.result()}, val_acc: {val_acc.result()}')\n",
    "    \n",
    "    val_acc_curve.append(val_acc.result())\n",
    "    val_loss_curve.append(val_loss.result())\n",
    "    loss_curve.append(train_loss.result())\n",
    "    acc_curve.append(train_acc.result())\n",
    "\n",
    "    if epoch == (EPOCHS-1):\n",
    "        with open(\"model_params_result.txt\",\"w\") as fichier:\n",
    "            fichier.write(f\"Batch Size : {BS} \\n --- \\n \")\n",
    "            fichier.write(f\"Epochs : {EPOCHS} \\n --- \\n \")\n",
    "            fichier.write(f\"Leraning_Rate : {L_R} \\n --- \\n \")\n",
    "            fichier.write(f\"Step per epochs : {STEP_PER_EPOCH} \\n --- \\n \")\n",
    "            fichier.write(f\"Step per epochs val : {STEP_PER_VAL_EPOCH} \\n --- \\n \")\n",
    "            fichier.write(f\"train_acc : {train_acc.result()} \\n --- \\n \")\n",
    "            fichier.write(f\"val_acc : {val_acc.result()} \\n --- \\n \")\n",
    "            fichier.write(f\"train_loss : {train_loss.result()} \\n --- \\n \")\n",
    "            fichier.write(f\"val_loss : {train_acc.result()} \\n --- \\n \")\n",
    "            fichier.write(f\"Optimizer : Adam \\n --- \\n \")\n",
    "            fichier.write(f\"Metrics/Loss : SparseCategoricalCrossEntropy/Accuracy \\n --- \\n \")\n",
    "            fichier.write(f\"Architecture : {dict_architecture} \\n --- \\n \")\n",
    "            fichier.write(f\"Taille dict : {len(dict_architecture)} \\n\")\n",
    "\n",
    "    val_loss.reset_states()\n",
    "    val_acc.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    train_loss.reset_states()\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "model.save(\"TikaiNetV1/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyze Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "plt.plot(val_acc_curve,c=\"red\",label=\"Val_Acc\")\n",
    "plt.plot(acc_curve,c=\"blue\",label=\"Accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(val_loss_curve,c=\"red\",label=\"Val_Loss\")\n",
    "plt.plot(loss_curve,c=\"blue\",label=\"Loss\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"plot.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TikaiNet = tf.keras.models.load_model('TikaiNetV1')\n",
    "\n",
    "i=0\n",
    "for X,y in X_test:\n",
    "    for image in X:\n",
    "        i+=1 \n",
    "        plt.figure()\n",
    "        plt.imshow(image)\n",
    "        image = image.reshape(1,224,224,1)\n",
    "        A = TikaiNet.predict(image)\n",
    "        print(A)\n",
    "        if i == 10:\n",
    "            break\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "4c65b98e956c6ae24f8ae0bc56d1e465ff92310dbdec0a4bd6b48ffdf1441c98"
  },
  "kernelspec": {
   "display_name": "Python 3.8.4rc1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.4rc1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
